## #1

Comments to Authors.
Overall, the paper makes many claims about the state of the art in
fake news detection that are not true. Additionally, paper makes many
assumptions about misinformation that limits the scope of the type of
misinformation this method can detect. Capturing emotions well can be
useful, but to understand the impact and limitations of this specific
model, the authors need to compare it to competing methods with
multiple broad based data sets.

Authors are categorizing microblog content not news. It is not clear
how the messages collected are categorized as news (and not news). How
about opinion pieces, are they considered news? How do you label
something as fake news? News may be outright false or it may be
misleading. Authors seem to rely on a single source's categorization
of it. How is this source's categorization justified?  What is the
source's method behind the categorization of something as fake?
Overall, the data collection is not well defined.

The paper makes many assumptions of how fake news differs from regular
news, assuming that capturing emotions is the most important thing for
this classification.

The idea that false and misleading news, including rumors on Twitter,
use more emotional language than not is well studied, and they cite
some papers that show this. But, they then claim that emotion has not
been utilized in fake news/rumor studies, which is untrue. In fact,
the majority of studies that use any set of hand crafted features use
emotion features, sometimes only emotion features, even going back to
2011.

Note that emotion is not necessarily sufficient to categorize all
"fake news" or "misinformation". There is a great deal of research in
this field that the authors seem not to be aware of.  There are
differences in terms of underlying incentives. If financial incentives
are involved, clickbait content is more common which does not target
emotion but curiosity. Recent work shows stylistic differences and
readability based features can be quite effective as well, in addition
to emotion markers. These are not part of the "hand crafted" features
used in this paper.

Authors use both the emotion in the tweet and the emotion in the
responses to the tweet to detect "fake news", claiming that "[fake]
publishers may present news objectively to make it
convincing... however, is controversial which evoke intense emotion in
the public." This claim that the responses to fake news display
specific emotions more than real news has not been shown anywhere. In
Figure 2, authors provide a distribution of different types of emotion
between fake and real news, which somewhat supports their point, but
we do not know if these differences are significant.

Finally, authors claim that the previous ways of capturing emotion in
text will not work well on social media, which is completely
false. Of the previous tools they mention, which is only a few,
there has not been any work showing these do not work. There are
tools made specifically for emotion/sentiment on social media
(i.e. Vader Sentiment). Because of this claim they use a previously
developed emotion embedding tool method that was used on product
reviews. This method probably works just fine, maybe even better, but
the claims about why they have to use it are not so true.

I recommend the authors review recent papers on this topic in WWW,
ICWSM and KDD. The one work they compare (Castillo 2011) is not
specifically built for "fake news" but for information credibility,
clearly quite old and not tested in today's media environment. There
are many and more recent methods that directly target fake news that
would be a better starting point for comparison.

## #2

Comments to Authors.
In this paper, the authors propose to study the fake news detection problem, and the authors claim that emotion can be the key factor for the detection of fake news. This paper introduces a deep learning model for the identification of the fake news articles, where the emotion factor is incorporated into the model.



This paper is well written and easy to follow. However the main concern is with the motivation of this paper. I cannot agree that emotion is the key to fake news detection. Many of the fake news articles actually release no emotion signals at all. For instance, given a short sentence "There is people living on the Mars." It is a neutral statement without any emotions, even though it is a fake news. Most of the well-designed fake news articles are not correlated with the emotion factors. It makes the motivation of this paper questionable.



As to the experiments, the authors fail to compare with many baseline methods, which are the state-of-the-art works on fake news detection. It is hard to judge the effectiveness of their model merely based on the current experiment results. It is suggested to add the comparison experiments with them.

[1] Yang et al. TI-CNN: Convolutional Neural Networks for Fake News Detection.

## #3

The paper proposes the novel idea of considering the emotions in user comments to a given news to detect its veracity. The paper aims to prove that adding this component improves over news content detection only.

The overall idea proposed in the paper is novel and well motivated, however there is still room for improvement, especially in the validation part, which makes the paper not ready for publication:

The new technique is evaluated on one dataset only, which makes the contribution limited. Since downloading a dataset similar to the one used in the paper on Twitter is feasible, the authors could introduce a new dataset in the experiment. It will generalize the validity of the proposed approach.
The authors do not compare with recent state of the art methods for fake news detection from the content, see for instance [2,3]. Castillo et al. 2011 is a quite old method and there are many researches done after that.
Other works have considered the social context (user shares and social network between the users - see, for instance [1]). The authors should discuss this approach and compare with it as well as they are somehow close each other.
The description on the news clustering in the first para of section 5.1 is confusing. I got the general meaning and motivation for that, but the explanation could be improved.


[1] Kai Shu, Suhang Wang, Huan Liu: Beyond News Contents: The Role of Social Context for Fake News Detection. WSDM 2019: 312-320.

[2] Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea.2018. Automatic Detection of Fake News. InProceedings of the 27th InternationalConference on Computational Linguistics. 3391–3401.

[3] Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and BennoStein. 2018. A Stylometric Inquiry into Hyperpartisan and Fake News. InProceed-ings of the 56th Annual Meeting of the Association for Computational Linguistics,ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers. 231–240.

## #4

Comments to Authors.
Very interesting paper on a important issue. The paper is well written and well motivated. The methodology is clearly described and a comparison with other methods is proposed. Some questions about the management of data arise : one knows very little about the way data is collected from Weibo. One may also wonder whether the use of hand crafted emotion taggind introduces a bias to handle news as training data. Some precisions may be added about the way emotional intensity is set. However these remarks do not put in question in the interest of the presented work.