Exploiting Emotions for Fake News Detection on
Social Media
Anonymous
Abstract‚ÄîMicroblog has become a popular platform for people
to post, share, and seek information due to its convenience
and low cost. However, it also facilitates the generation and
propagation of fake news, which could cause detrimental societal
consequences. Detecting fake news on microblogs is important for
societal good. Emotion is a significant indicator while verifying
information on social media. Existing fake news detection studies
utilize emotion mainly through users stances or simple statistical
emotional features; and exploiting the emotion information from
both news content and user comments is also limited. In the
realistic scenarios, to impress the audience and spread extensively,
the publishers typically either post a tweet with intense
emotion which could easily resonate with the crowd, or post a
controversial statement unemotionally but aim to evoke intense
emotion among the users. Therefore, in this paper, we study the
novel problem of exploiting emotion information for fake news
detection. We propose a new Emotion-based Fake News Detection
framework (EFND), which can i) learn content- and commentemotion
representations for publishers and users respectively;
and ii) exploit content and social emotions simultaneously for
fake news detection. Experimental results on real-world dataset
demonstrate the effectiveness of the proposed framework.
Index Terms‚ÄîFake News Detection, Gate Mechanism, Emotion
Embedding
I. INTRODUCTION
Social media platforms play a crucial role for people to
seek out and spread information, especially in emergencies and
breaking news. However, the convenience of publishing and
spreading information also foster the wide propagation of fake
news, commonly referred as intentional false information [1].
For instance, an authoritative analysis of BuzzFeed News1
indicated that, during the 2016 U.S. presidential election
campaign, top 20 fake news stories generated more total
engagement on Facebook than top 20 major real news, that
these fake news earned nearly 9 million shares on social media.
These fakes news seriously do harm to not only the public
credibility, but also social stability and economic market.
Therefore, it‚Äôs significantly important to build tools to detect
the fake news automatically and effectively.
Existing works on fake news detection mainly focus on
news content and social context. Feature-based classification
models extract basic semantic and emotion features from
content, and statistical features from users [2]. Propagationbased
model construct the relationship network inside the
Identify applicable funding agency here. If none, delete this.
1https://www.buzzfeednews.com/article/craigsilverman/viral-fake-electionnews-
outperformed-real-news-on-facebook
(a) (b)
Fig. 1: Two fake news posts from Sina Weibo. (a) a post
which contains emotions of astonishment and sadness in news
contents that easily arouse the audience. (b) a post which
contains no emotion, but raise emotions like doubt and anger
in user comments by controversial topics.
event, and incorporate social conflicting viewpoint towards the
event in the network [3], [4]. Recently, deep learning models
are proposed to evaluate the credibility of information on
social media, which deeply exploits the semantics information
from news content [5]. Basic social context features are fused
into deep learning models in some studies [6]. However, emotion
information, which is crucial for fake news detection, is
underutilized in these studies. Fewer studies leverage emotion
in news contents and social context simultaneously for fake
news detection.
Fake news publishers often aim to spread information extensively
and draw wide public attention. Longstanding social
science studies demonstrate that the news which evokes higharousal,
or activating (awe, anger or anxiety) emotions is more
viral on social media [7]‚Äì[11]. To achieve this goal, fake news
publishers commonly adopt two approaches. First, publishers
post news with intense emotions which trigger a high level of
physiological arousal in the crowd. For example, in Figure 1a,
the publisher uses rich emotional expressions (e.g., ‚ÄúOh my
god!‚Äù) to make this information more impressive and striking.
Second, publishers may present the news objectively to make
it convincing whose content, however, is controversial which
evoke intense emotion in the public, and finally spreads widely.
As another example (see Figure 1b) shows, the publisher
writes the post in a unemotional way; while, the statement
that China ranks second to the last suddenly bring on tension
in the crowd, and people express their feeling of anger (e.g.,
‚Äúmost ridiculous‚Äù), shock and doubt (e.g., ‚Äúseriously?‚Äù) in
comments. Therefore, learning emotion of the publisher and
users corporately has the potential to improve fake news
detection performance.
This circumstance is also consistent with the realistic
dilemma of exploiting emotion. Since a large part of fake
news doesn‚Äôt contain emotional signals in contents, difference
of emotion between fake news and real news content maybe
not obvious. However, this situation couldn‚Äôt undermine the
importance of emotion. To impress audience, these fake news
content tends to be more controversial and inspiring, though
unemotional. We could mine the emotion information from
the audience, such as reposts or comments, to capture this
characteristics. Majority of existing works utilizing emotion
information for detecting fake news either extract emotion
feature from news content [2], [6], or model the viewpoints
of users in propagation [4], [12].
To exploit emotion information for fake news detection,
we first define two types of emotion: (1) publisher emotion:
emotion of the publisher while posting information on social
media; and (2) social emotion: emotion of users when the
information disseminates on social media. In this paper, news
content and user comments are used to capture publisher
emotion and social emotion respectively. In essence, we investigate:
(i) how to capture signals of publisher emotion
and social emotion from news content and user comments,
respectively; (ii) how to exploit publisher and social emotions
simultaneously for fake news detection. Our solutions to
these two challenges results in a novel Emotion-based Fake
News Detection framework (EFND). In practice, we leverage
emotion embedding for better emotional representation of each
word. And various gates are designed to exploit information
from different modules simultaneously. Our main contributions
are summarized as follows:
 We provide a principled way to capture the publisher
emotion and social emotion signals, and demonstrate the
importance of these two emotions from various perspectives
on fake news detection.
 We propose a novel framework EFND, which exploits
a deep neural network to learn representations from
publisher emotion, social emotion and content simultaneously,
for fake news detection.
 We conduct experiments on real world datasets to show
the effectiveness of EFND for fake news detection.
The paper is organized as follows. In the next section,
we would give a brief overview of works in related areas.
After that, we demonstrate the difference of emotion between
fake news and real news with analysis on dataset. Section
IV presents the details of the framework EFND, including
emotion embedding and various gates. Datasets, experiment
settings and results are presented in Section V. We conclude
our work in Section VI.
II. RELATED WORK
We briefly describe the related work from three-folds: i)
Fake News Detection; ii) Emotion Representation; and iii)
Multi-modal Fusion.
A. Fake News Detection
Previous fake news detection studies mostly focus on extracting
features and training a classifier to predict the credibility
of news. Early work manually extracts a wide range of
features including user features, content features, propagation
features and topic features [2]. And location and client are
proved to be effective in detecting fake news [13]; Besides
feature-based models, propagation-based approaches aim to
mine the relations between various entities in a event. Propagation
network is firstly introduced in credibility evaluation
by constructing a relation network in work [14]. Tweet, user,
event and their inter-relations construct a hierarchical network,
which is able to evaluate the credibility of each tweet and
event by iterative calculation. Similar hierarchical structure
is applied on microblogs which consists of news, sub-events
and messages [3]. And conflicting viewpoints among users are
further considered in the propagation network [4]. Recently,
neural network models are adopted for fake news detection. [5]
firstly applies RNN for fake news detection on social media,
modeling the posts in a event as a sequential time series. [6]
proposes a social attention network to capture the hierarchical
characteristic of events on microblogs.
So far, emotion in these works is used as either emotion features
or viewpoints of users, which requires more systematical
and comprehensive explorations in future work.
B. Emotion Representation
Early studies primarily use hand-crafted features for representing
emotion of word, which highly rely on sentiment
dictionaries. There are several widely-used emotion dictionaries,
including WordNet [15], LIWC2, MPQA [16]for English,
and HowNet 3 for Chinese. Beyond this, many works leverage
these dictionaries and man-made rules to mine emotion information.
For example, Vader [17] constructs a list of lexical
features and rules to assess the sentiment of microblog-like
contents. However, this method could only align each sentiment
word with an emotional class, or intensity value, which
is somewhat not informative for representing words‚Äô emotion
information. Besides, low coverage and differences of word
usage on social media and in real world limit the effectiveness
of sentiment dictionaries in some circumstances. Learning
task-specific emotion embedding with neural network could
encodes sentiment information in the continuous representation
of words, which has been proved to be effective recently.
Tang et al. [18] utilizes a massive distant-supervised tweet
dataset to obtain emotion embedding on social media. Agrawal
2www.liwc.net
3http://www.keenage.com/html/e index.html
et al. [19] learns emotion-enriched word-representation on
product reviews, with a much smaller corpus. This method
enables each word to obtain a distributed representation vector
which contains rich and high-level emotion information.
C. Multimodal Fusion
The simplest approach is concatenation, which fuses different
modality vectors by concatenation. [20] employ a stacked
autoencoder to learn multimodal representations by embedding
linguistic and visual inputs into a common space. However,
these methods treat the different modalities equally in fusion.
In [21], the authors employ the high-level representation of
visual modal as the external attention vector to weigh the
components of textual modality. Gate mechanism is also
widely used in many fusion work. LSTM [22] and GRU
[23] tackle the long-term dependency problems of RNN with
different gates that could control how information from last
time step and current inputs updates at current unit, which is
another form of fusion. [24] deploys gates to learn the weights
of different modality representations for each word.
Considering the importance of emotion in fake news detection,
we proposed a novel fake news detection framework
which could exploit emotion information from the publisher
and users with the help of emotion embedding and gates.
III. A PRELIMINARY ANALYSIS ON EMOTION SIGNALS
To explore the relationships between emotions and fake
news on social media, we perform a preliminary data analysis
on emotions signals for fake and real news in news content
and user comments respectively. We collect the dataset from
a popular microblog platform Weibo 4 (details of data preparation
are introduced in Sec V-A). We have collected 7,880
fake news pieces and 7,907 real news pieces, and their related
user comments on Weibo. The analysis is performed from
three perspectives: 1) the emotional category; 2) the emotional
intensity; and 3) the emotional expression.
A. Emotional Category
Generally, fake news is sensational and inflammatory. It
could arouse specific kinds of emotion among the users,
such as suspicion, anxiety or shock. Therefore, we select
5 fine-grained emotion categories to investigate emotions in
fake news and real news, including anger, sadness, doubt,
happiness and none (Some contents may not contain emotional
information). We adopt the emotion classifier in Sec IV-A0b
to annotate our experimental data with emotion categories.
Figure 2a and 2b exhibit the distribution of emotion categories
in news content and user comments respectively. In
fake news‚Äô content, the proportion of anger is 9% higher
than it in real news, while the percentage of happiness is 8%
lower. Same trend happens in the user comments. In addition,
the proportion of sadness in fake news‚Äô contents and doubt
4www.weibo.com
in fake news‚Äô comments is much higher than them in real
news. The result demonstrates that, compared to in real news,
both the publisher and users tend to express more high-arousal
emotions, such as anger, sadness and doubt in fake news.
(a) News Content (b) User Comments
Fig. 2: Distributions of emotional category of fake news and
real news in: (a) news content and (b) user comments. Anger
is more likely to appear in fake news, while real news arouse
more happy emotion in both sources
B. Emotional Intensity
Each document also owns an emotional intensity level in
each emotional category. For example, the intensity of I‚Äôm
super happy is much stronger than intensity of I‚Äôm happy
in happiness emotional category. Fake news is expected to
express negative emotion with stronger intensity which could
further arouse intense emotions in the public. In this section,
we take the output probability of emotional category classifier
models(in Sec IV-A0b) softmax layer as the emotional intensity
level for each emotional category, which is a continuous
value between 0 and 1.
In Figure 3, we can see that, regardless of sources, the
emotions of anger, sadness and doubt in fake news are much
severer than in real news. And this discrepancy is more drastic
in news content. In conclusion, both the publisher and users
are more possible to express stronger negative emotions in fake
news than in real news, while the trend of positive emotion is
reverse.
(a) News Content (b) User Comments
Fig. 3: Distributions of emotional intensities level of fake news
and real news in: (a) news content and (b) user comments. The
intensities of emotion anger, sadness and doubt in fake news
are all stronger than in real news.
C. Emotional Expression
Different people may express their feelings with different
linguistic usage. For example, some people like using plain
words to express their feelings, while others prefer exaggerated
words. In fake news, inciting words might be more
preferred due to the controversial news content. To analyze
the differences of emotional expression, we extract the topweighted
words for expressing anger in real news and fake
news respectively. We adopt the widely-used method in [25] to
calculate the weight of each word in the dataset for expressing
specific kinds of emotion. The top-weighted 30 words in fake
news and real news are shown in Figure 4.
(a) Fake News (b) Real News
Fig. 4: Emotional expressions for anger in fake news and real
news. Compared to real news, fake news use more fierce and
extreme words to express anger.
We can see that fake news conveys angry with much more
fierce and extreme words like ‚Äùdamn it‚Äù, ‚Äùass‚Äù. Similar circumstance
also exists in other negative emotional categories,
which are not shown here. Therefore, people use different
words for emotional expression in fake and real news.
In summary, we make the following conclusions from these
experiments: i) both the publishers and users are more likely to
spread more negative emotions in fake news than in real news;
ii) participants of fake news tend to express negative emotions
with stronger intensities; iii) while expressing a specific kind
of emotion, people in fake news prefer exaggerated and
inflammatory words.
IV. MODELING EMOTIONS FOR FAKE NEWS DETECTION
In this section, we present the details of the proposed
end-to-end emotion based fake news detection framework
EFND. It consists of three major components (see Figure 5):
i) the content module mines the information from the publisher,
including semantics and emotions information in news
contents; and ii) the comment module captures semantics
and emotion information from users; and ii) the fake news
prediction component fuses the high-level features from both
news content and user comments, and then predict fake news.
A. Content Module
News contents contain the cues to differentiate fake and
real news. We have shown that the distributions of emotional
categories are different for fake and real news pieces, which
demonstrate the potential to use news content emotions to help
detect fake news.
a) Word Encoder: We learn the basic textual feature
representations through a recurrent neural network (RNN)
based word encoder. Though in theory, RNN is able to
capture long-term dependency, in practice, the old memory
will fade away as the sequence becomes longer. To make it
easier for RNNs to capture long-term dependencies, Gated
Recurrent Units (GRU) [23] are designed in a manner to have
more persistent memory. To further capture the contextual
information of annotations, we use bidirectional GRU to model
word sequences from both directions of words. For each word
ti, the word embedding vector wi is initialized with the pretrained
word2vec [26]. The bidirectional GRU contains the
forward GRU
ÙÄÄÄ!f
which reads each sentence from word t0 to
tN and a backward GRU
 ÙÄÄÄ
f which reads the sentence from
word tN to t0:
ÙÄÄÄ!
hwi
=
ÙÄÄÄÙÄÄÄÙÄÄÄ!
GRU(wi); i 2 [1;N];
 ÙÄÄÄ
hwi
=
 ÙÄÄÄÙÄÄÄÙÄÄÄ
GRU(wi); i 2 [1;N]:
(1)
for a given word ti, we could obtain its word encoding
vector hwi
by concatenating the forward hidden state
ÙÄÄÄ!
hwi
and
backward hidden state
 ÙÄÄÄ
hwi
, i.e., hwi
= [
ÙÄÄÄ!
hwi
;
 ÙÄÄÄ
hwi
]
b) Emotion Encoder: Similar to the word encoder, we
adopt bidirectional GRU to model the emotion feature representations
for the words. To preserve the emotion signal
for each word, we first introduce how to obtain an emotion
embedding vector teifor each word ti.
Inspired by recent advancements on deep learning for emotion
modeling [19], we train a recurrent neural network to
learn the emotion embedding vectors. Following traditional
settings [27], we first obtain a large-scale Weibo dataset in
which each Weibo contains emoticons. Then we categorize
the top 200 emoticons into 5 emotional classes(anger, doubt,
happiness, anger, sadness and none ), and use the emoticons
to label the corpus. Technically, we initialize each word
with the one-hot vector. Here we don‚Äôt use pre-trained word
embeddings for not learning too much semantic information
in emotion embeddings. After initiation, all words pass an
embedding layer which projects each word from the original
one-hot space into a low dimensional space, and then are
sequentially fed into a one-layer GRU model. Finally, through
back-propagation, the embedding layer gets updated during
training, producing emotion embedding ei for each word ti.
Besides, this classifier is also used for analysis in Sec III. For
experiment on Twitter, we perform the same procedure on the
labeled emotion tweet corpus which is published in SemEval
2018 [28], to get emotion embeddings of English words.
After we obtain the emotion embedding vectors, we can
learn the emotion encoding hei
for word ti:
ÙÄÄÄ!
hei
=
ÙÄÄÄÙÄÄÄÙÄÄÄ!
GRU(ei); i 2 [1;N];
 ÙÄÄÄ
hei
=
 ÙÄÄÄÙÄÄÄÙÄÄÄ
GRU(ei); i 2 [1;N]:
(2)
Fig. 5: The proposed framework EFND consists of three components: (1) the news content module; (2) the user comments
module, and (3) the fake news prediction component. The previous two modules are used to model semantics and emotions
from the publisher and users respectively, while prediction part fuses information of these two module and make prediction.
Three gates at the right side are used for multimodal fusion at different layers in this framework.
for a given word ti, we could obtain its emotion encoding
vector hei
by concatenating the forward hidden state
ÙÄÄÄ!
hei
and
backward hidden state
 ÙÄÄÄ
hei
, i.e., hei
= [
ÙÄÄÄ!
hei
;
 ÙÄÄÄ
hei
].
c) Hand-crafted News Emotion Features: The overall
emotion information of news content is also important when
deciding how much information from emotion part should be
absorbed for the words inside the news piece. For example, the
news which obviously express intense emotions could further
strengthen the importance of emotion part on its words in
training process. For a given post pj , we extract the emotion
features included in [2] and also add some extra emoticon
features. There are 19 features regarding emotion aspects of
news, including numbers of positive/negative words, sentiment
score, etc. News emotion features of pj is denoted as sej .
d) News Content Representation: Gate N is applied to
learn information jointly from word embedding, emotion
embedding and sentence emotion features, and then yield
new representation for each word(see Figure 5). The units
in Gate N is motivated by the forget gate and input gate in
LSTM. In Gate N, two emotion inputs corporately decide the
value of rt and ut with two sigmoid layers, which are used for
managing how much information from semantic and emotion
is added into the new representation. Meanwhile, a tanh layer
transfer the emotion inputs to the same dimensional space
of word embedding. Mathematically, the relationship between
inputs and output of Gate N is defined as the following
formulas:
rt = (Wr:[se; het
] + br)
ut = (Wu:[se; het
] + bu)
cet
= tanh(Wc:[se; het
] + bc)
nt = rt  hwt
+ ut  cet
(3)
All the generated vectors of words are fed into a bidirectional
GRU layer sequentially, and then the last hidden state
of the GRU layer is expected to contain all the information in
Content Module, called Content Representation.
B. Comment Module
Comment module explores the semantic and emotion information
from users in the event. The architecture of comment
module is similar to content module‚Äôs except: 1) all comments
are firstly concatenated before fed into BiGRU; 2) there is no
sentence emotion features; and 3) Gate C is used for fusion.
We choose to concatenate all the comments for inputs because
over 70% news pieces own less than 5 comments, which
reflect the situation in real world as well. As a consequence,
the input doesn‚Äôt own the intact information as a sentence, so
there is no sentence emotion features.
Gate C is introduced for fusion in comment module. Different
from Gate N, there are only two modalities. We adopt
the update gate in GRU to control the update of information
in fusion process (see Figure 5). Two inputs jointly yield
a update gate vector utthrough a sigmoid layer. A tanh
layer create a vector of new candidate values, cet
, which has
the same dimension as the hwt
. The final output nt is a
linear interpolation between the cet
and hwt
. Mathematically,
following formulas represent the process:
ut = (Wu:[hwt
; het
] + bu)
cet
= tanh(Wc:het
+ bc)
nt = ut  hwt
+ (1 ÙÄÄÄ ut)  cet
(4)
C. The proposed Framework - EFND
Here, Gate M fuse the high-level representation of content
module and comment module, and then yield a representation
vector N(see Figure5). Mathematically, following equations
demonstrate the internal relationship of Gate M:
r = (Wu:[con; com] + bu)
N = r  con + (1 ÙÄÄÄ r)  com
(5)
We use a fully connected layer with softmax activation to
project the new vector N into the target space of two classes:
fake news and real news, and gain the probability distribution:
p = softmax(WcN + bc) (6)
In the proposed model, we employ a binary-entropy function
to define the loss of the m-th sample Sm as follow:
L(Sm) = ÙÄÄÄ[lmpm + (1 ÙÄÄÄ lm) log(1 ÙÄÄÄ pm)] (7)
where pm denotes the probability of being fake news of m-th
sample, and lm denotes the ground truth of m-th sample with
1 representing fake news and 0 representing real news.
V. EXPERIMENT
In this section, experiments on two real-world datasets on
social media are conducted to evaluate the effectiveness of
EFND. Specifically, this section aims to answer the following
questions:
 Q1: Is EFND capable of detecting fake news on social
media? And compared to state-of-art methods, how much
performance could it improve?
 Q2: How effective is emotion and emotion embedding in
improving the performance of EFND?
 Q3: How effective are the three proposed gates in improving
the performance of EFND? And how do they
achieve this?
We firstly introduce the datasets that are used in experiments,
and the experiment settings including implementation details
and some representative methods as baseline models. Then
we compare EFND with these methods to answer Q1, and
we conduct ablation study on emotion to answer Q2. We
conduct ablation study on gates and extract the weight vectors
in various gates to investigate what is exactly learned though
gates, which answers Q3.
A. Datasets
a) Weibo Dataset: We construct a dataset on Sina Weibo.
This dataset includes 7880 fake news pieces and 7907 real
news pieces, with nearly 160k comments. The fake news
pieces are directly collected from the official rumor debunking
system of Weibo5. This system actually serves as an authoritative
source to collect fake news in many literatures [5], [21].
And the real news pieces are gathered from NewsVerify6, a
real -time news certification system on Weibo which contains
a large-scale verified truth posts on Weibo [29]. All user
comments in 24-hours time interval after news publishing time
are collected for each news post. Note that not every post owns
comments.
b) Twitter Dataset: For Twitter7 dataset, we use the
dataset built in [5]. All fake news are crawled for Twitter by
searching keywords extracted from fake news on Snopes. Part
of non-rumor events are also from Snopes, and others are from
two public datasets [2], [30]. This dataset contains 498 fake
news pieces and 494 real news pieces. All the reposts of these
news pieces are collected as well, which are used to mine the
information from users in our experiments.
The statistics of experiment datasets are as Table I. In our
experiment, we first use K-means algorithm to cluster all
news into 200 clusters, and split them into training data and
testing data in ratio 4:1 at cluster level. Trough this way could
we promise that there is no event topic overlap between the
training and testing sets, which prevent model from overfitting
on event topics.
Weibo Twitter
# Source Posts of Fake News 7,880 498
# Source Posts of Real News 7,907 493
# All Posts of Dataset 15,787 992
# User Comments/Reposts of Fake News 109,154 134,590
# User Comments/Reposts of Real News 47,037 373,500
# All Comments/Reposts of Dataset 156,191 508,090
TABLE I: The Statistics of Experiment Dataset.
B. Compared Fake News Detection Methods
For word embedding, we align each word with a 32-
dimensional vector which is from a pre-trained Word2Vector
model on each dataset, and with a 16-dimensional emotion
embedding vector from the pre-trained embedding layers(
See SecIV-A0b). We set the dimension of the hidden
states of Bi-GRU as 32 and use Adam [31] for stochastic
optimization. Batch size of the training process is 128. DO
WE NEED to explain that we only use up to 5 comments for
each news piece??
We compare our framework with the following state-of-art
methods:
5http://service.account.weibo.com/
6https://www.newsverify.com/
7http://www.twitter.com
 DTC Catillo et al. [2] uses J48 decision tree to evaluate
the credibility of of tweets with hand-crafted features.
The features also include basic emotion features.
 ML-GRU Ma et al. [5] model a post event as a variablelength
time series and apply a multilayer GRU network
to learn these pieces. Since there is no reposts in our
dataset, we take the comments as a replacement.
 Basic-GRU contains two generic Bi-GRU network to
model semantics of news content and comments with
word embedding, with simple concatenation on top layer.
 HSA-BLSTM Guo et al. [6] use a hierarchical attention
model to capture the hierarchical structure in a post
event. Social context features is incorporated in the model
through attention mechanism which also contain some
basic emotion features. Similarly, we use comments as a
replacement of reposts while implementation.
 CSI Ruchansky et al. [32] proposes a framework which
is composed of three modules that integrate the response,
text and users to classify an article as fake or not. This
model uses LSTM to capture the temporal representation
of articles, a fully connected layer to model characteristics
of users and then concatenate these two vectors for
classification.
We follow the conventional metrics: Accuracy, Precision,
Recall, and F1-Score for a comprehensive evaluation.
C. Performance Comparison
Table II presents the experiment results of all compared
methods and the proposed model. In particular, the EFND
model achieves an overall accuracy of 87.2% on weibo dataset
and 75.1% on Twitter dataset, which both outperform all the
baseline models. The outstanding performance of the proposed
model demonstrates that incorporation of emotion through
embedding representation and gated fusion could effectively
promote the detecting process on fake news.
We can see that all the neural network models earn better
performance than the hand-crafted feature based methods. This
may indicate that generic RNN is capable of exploiting deep
latent features of text through variable time-series architecture.
And the Basic-GRU model outperforms ML-GRU on both
dataset mainly because that the amount of responses are
somewhat too small to support the complicated structure of
ML-GRU which rely on rich repost sources.
Our method shows its strength on fake news detection in
these experiments. As is shown in Table II, on Weibo dataset
EFND rise the accuracy of fake news detection by nearly
12%, from 75.6% of decision tree to 87.2%. And its f1-
score is also 4% higher than the second one. On Twitter
dataset, the improvement is much more obvious by boosting
the accuracy from 61.3% of feature-based models to 75.1%.
As well, EFND outperform the second-best model by over 7%
in F1-Score. These observations demonstrate the importance
of incorporating emotion information into models.
Dataset Methods Accuracy Precision Recall F-1
Weibo
DTC 0.756 0.754 0.758 0.756
ML-GRU 0.799 0.810 0.790 0.800
Basic-GRU 0.835 0.830 0.850 0.840
CSI 0.835 0.735 0.996 0.858
HSA-BLSTM 0.843 0.860 0.810 0.834
EFND 0.872 0.860 0.890 0.874
Twitter
DTC 0.613 0.608 0.570 0.588
ML-GRU 0.684 0.663 0.740 0.692
Basic-GRU 0.695 0.674 0.721 0.697
CSI 0.696 0.706 0.649 0.671
HSA-BLSTM 0.718 0.731 0.663 0.695
EFND 0.751 0.698 0.860 0.771
TABLE II: Performance Comparison of Fake News Detection.
D. Component Analysis
To analyze the effectiveness of emotion and gates in content,
comments and the whole framework respectively, we take out
the content module alone, comment module alone, and the
whole framework for ablation study experiments.
We use WE and EE to denote that only word embedding
or emotion embedding is used. WEE means that both of
these two embeddings are used. Meanwhile, symbols c, gn,
gc, gm are representing fusion strategies simple concatenation,
Gate N, Gate C and Gate M respectively. Symbol
att denotes attention fusion strategy in [21], which take the
high-level representation of one modality as external attention
vector to weigh the components of another modality.
Table III reports the experiment results of different modules
on two datasets.
a) Emotion Signals: From Table III. We could make
the following observations: 1) in content module, the overall
performance rises while using emotion embedding; Especially
on Twitter dataset, adding emotion information increases the
F1 Score by over 7%; 2) emotion play a more important role
in content module than comment module on our dataset. It
possibly results from the sparsity of comments data, which
limits the effectiveness of emotion in comment module. 3)
compared to merely using semantic information, incorporation
of emotion from one side or two sides all improve the
performance of the whole framework, which demonstrate the
importance of emotion on fake news detection;
b) Gate Mechanism: We take out the content module
alone, comment module alone, and the whole framework for
experiments, by using different fusion strategies. Here we
compare our gate with simple concatenation and attention
fusion. As is shown in Table III, various gates further improve
the promotion that emotion information brings on classification.
In particular, Gate N in content module evidently
increase the F1-Score by around 4% compared to simple
Module Methods Weibo Dataset Twitter Dataset
Accuracy Precision Recall F-1 Accuracy Precision Recall F-1
Content
Module
WE 0.790 0.758 0.849 0.801 0.678 0.716 0.558 0.627
EE 0.700 0.670 0.760 0.719 0.639 0.646 0.609 0.615
WEE(c) 0.813 0.793 0.826 0.810 0.690 0.650 0.779 0.709
WEE(att) 0.799 0.788 0.798 0.793 0.701 0.714 0.640 0.675
WEE(gn) 0.851 0.835 0.873 0.854 0.725 0.687 0.791 0.735
Comment
Module
WE 0.667 0.846 0.407 0.550 0.667 0.680 0.593 0.634
EE 0.619 0.667 0.472 0.553 0.655 0.629 0.709 0.667
WEE(c) 0.669 0.831 0.423 0.560 0.689 0.667 0.721 0.693
WEE(gc) 0.671 0.836 0.424 0.563 0.713 0.701 0.709 0.705
Content
+ Comment
(WE+WE)(c) 0.835 0.830 0.850 0.840 0.695 0.674 0.721 0.697
(WEE(gn)+WE)(c) 0.863 0.860 0.870 0.860 0.736 0.686 0.837 0.754
(WEE(gn)+WEE(gc))(c) 0.866 0.830 0.920 0.870 0.746 0.678 0.907 0.776
(WEE(gn)+WEE(gc))(gm) 0.872 0.860 0.890 0.874 0.751 0.698 0.860 0.771
TABLE III: Component Analysis of Emotion Embedding.
concatenation, and nearly 5% while contrasting with attention
fusion on Weibo dataset. On the other hand, the improvement
brought by Gate C and Gate M is not as obvious as Gate N,
at less than 1% on both datasets.
(a)
(b)
Fig. 6: The distribution of weights calculated by Gate C
between semantic and emotion of each word in two sentences.
E. Case Study
To further investigate what is actually learned inside gates,
we extract the vector ut in Gate C which is a weight vector
between semantic vector hwt
and emotion vector cet
for each
word. We compute the average of the vector as an approximation
of the weight between two modalities. Figure 6 shows
two examples in fake news. We could observe that: 1) emotion
in sentiment words such as ‚Äùscary‚Äù, ‚Äù!‚Äù , ‚Äù?‚Äù and ridiculous
gain higher weight than semantic information. Many of these
words even don‚Äôt appear in sentiment dictionaries; and 2)
sentiment words‚Äô emotion modality obtain more attention than
unemotional words.
Fig. 7: Fake news whose comment modules are top weighed
by Gate M.
Similarly, we also compute the average of vector r as an approximation
of weights between content module and comment
module in Gate M. Figure 7 show 5 fake news pieces whose
comment modules are top-weighted. Interestingly, most of
these news pieces‚Äô comments contains clues for verifying the
truth of news content(e.g.,‚Äùdeceived‚Äù, ‚Äùrumor‚Äù). This validates
the capability of Gate M on capture important knowledge
while fusing different modalities.
VI. CONCLUSION
In this paper, we propose a end-to-end emotion-based
fake news detection framework, EFND, which incorporate
publisher emotion and social emotion in fake news detection
simultaneously. We apply content module and comment
module to exploit semantic and emotion information from
the publisher and users respectively. Technically, we adopt
embedding to capture emotion information for each word. To
fully explore emotion in news event, three types of gates are
proposed for fusion at different levels in EFND. Extensive
experiments on Weibo and Twitter datasets demonstrate that
the proposed EFND model is effective for detecting fake
news and outperforms several state-of-art fake news detection
methods.